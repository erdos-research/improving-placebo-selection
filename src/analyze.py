# -*- coding: utf-8 -*-
################################################################################
# Analyze placebos generated by GPT-2 and GPT-3.5 Turbo
# Generates CSV file of sentiments and analysis file with relevant statistics
# Code authored by William W. Marx (marx.22@dartmouth.edu)
# Licensed under CC0 1.0 Universal
# The below code is released in its entirety into the public domain
# Please visit SOURCES.md for further attribution
################################################################################
import csv
from pathlib import Path
from subprocess import call
from typing import Iterable, List, Tuple, Union

try:
	import slant
	import numpy as np
	from multiprocess import Pool, cpu_count
	from scipy.stats import gaussian_kde
	from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
except ModuleNotFoundError:
	print("\033[31mMissing necessary requirements.\033[0m")
	# Ask user if they want to install missing packages and if so, do it
	if input("Install `slant`, `numpy`, `multiprocess`, `scipy`, and `vaderSentiment` now? [Y/n]: "
	        ).strip().lower() != "n":
		call("python3 -m pip install -U -r requirements.analyze.txt".split(" "))
	else:
		raise ModuleNotFoundError("Must install `slant`, `numpy`, `scipy`, and `vaderSentiment`")

# Get current working directory
cwd = Path(__file__).parent.resolve()

################################################################################
# Load CSV file containing GPT-2-generated placebos
################################################################################
gpt2_placebo_csv = (cwd / "../results/GPT2_generated_placebos.csv").resolve()
if not gpt2_placebo_csv.exists():
	raise Exception(  # Exit if no GPT-2 placebo results file
	    "\n".join([
	        "\033[31mGPT-2 placebo file (../results/GPT2_generated_placebos.csv) not found.\033[0m",
	        "Please run `python3 compile.py` to generate results file."
	    ]))

with open(gpt2_placebo_csv, "r") as f:
	gpt2_placebos = list(csv.reader(f))[1:]  # Load GPT-2 genearted placebos

gpt2_pdict = {" ".join(row[:2]): row[2:] for row in placebos}  # Build placebo dictionary

################################################################################
# Load CSV file containing GPT-3.5 Turbo-generated placebos
################################################################################
gpt3_5_placebo_csv = (cwd / "../results/GPT3_5_generated_placebos.csv").resolve()
if not gpt3_5_placebo_csv.exists():
	raise Exception(  # Exit if no GPT-3.5 Turbo placebo results file
	    "\n".join([
	        "\033[31mGPT-3.5 Turbo placebo file (../results/GPT3_5_generated_placebos.csv) not found.\033[0m",
	        "Please run `python3 compile.py` to generate results file."
	    ]))

with open(gpt3_5_placebo_csv, "r") as f:
	gpt3_5_placebos = list(csv.reader(f))[1:]  # Load GPT-3.5 Turbo genearted placebos

gpt3_5_pdict = {" ".join(row[:2]): row[2:] for row in placebos}  # Build placebo dictionary

################################################################################
# Get placebo sentiments
################################################################################
vader = SentimentIntensityAnalyzer()
get_sentiment = lambda r: [*r[:2], *map(lambda x: vader.polarity_scores(x)["compound"], r[2:])]


def get_placebo_sentiments(row):
	"""Transform row from placebo CSV file into row of corrensponding sentiments."""
	sentiments = map(lambda x: vader.polarity_scores(x)["compound"], row[2:])
	return [*row[:2], *sentiments]


# Get placebo sentiments in parallel
p = Pool(cpu_count())
gpt2_sentiments = p.map(get_sentiment, gpt2_placebos)
gpt3_5_sentiments = p.map(get_sentiment, gpt3_5_placebos)
p.close()

# Write GPT-2 sentiments to file
with open((cwd / "../results/GPT2_placebo_sentiments.csv").resolve(), "w") as f:
	csvw = csv.writer(f)
	max_length = max([len(row[2:]) for row in gpt2_placebos])
	csvw.writerow(["Identifier0", "Identifier1", *[f"Sentiment{i}" for i in range(max_length)]])
	csvw.writerows(list(sorted(gpt2_sentiments, key=lambda x: " ".join(x[:2]).lower())))

# Write GPT-3.5 Turbo sentiments to file
with open((cwd / "../results/GPT3_5_placebo_sentiments.csv").resolve(), "w") as f:
	csvw = csv.writer(f)
	max_length = max([len(row[2:]) for row in gpt3_5_placebos])
	csvw.writerow(["Identifier0", "Identifier1", *[f"Sentiment{i}" for i in range(max_length)]])
	csvw.writerows(list(sorted(gpt3_5_sentiments, key=lambda x: " ".join(x[:2]).lower())))


################################################################################
# Gather statistics on sentiment and substantive content
################################################################################
class MultimodalStats:
	"""Statistics for multimodal series"""

	def __init__(self, series: Iterable[float]) -> None:
		"""Initialize multimodal stats class"""
		self.series = np.sort(np.array(series))
		self._densities = self._get_densities()
		self.psi, self.subseries = self._get_psi_and_subseries()
		self.deviations = [self._deviation(s, p) for s, p in zip(self.subseries, self.psi)]

	def _get_densities(self) -> Iterable[float]:
		"""Estimate kernel-density at each point in series using Gaussian kernels"""
		return gaussian_kde(self.series).evaluate(self.series)

	def _get_psi_and_subseries(self) -> Tuple[Iterable[float], Iterable[Iterable[float]]]:
		"""Get both local maximums of density series and split series into subseries by its local
				minumums
				"""
		dprime = np.diff(self._densities)  # Derivative of density series
		local_mins = [0, len(self._densities - 1)]  # Indices where series will be split
		local_maxs = []  # Indices of psi's
		for i in range(1, len(dprime)):
			d0, d1 = dprime[i - 1:i + 1]
			if d0 > 0 and d1 < 0:
				local_maxs.append(i - 1)
			elif d0 < 0 and d1 > 0:
				local_mins.insert(-1, i - 1)  # Insert before last element
		psi = [self.series[i] for i in local_maxs]
		subseries = [self.series[local_mins[i]:local_mins[i + 1]] for i in range(len(local_mins) - 1)]
		return (psi, subseries)

	@staticmethod
	def _deviation(series: Iterable[float], reference: float) -> float:
		"""Roughly equivalent to standard deviation, but replaces mean with given reference value"""
		return np.sqrt(sum([(x - reference)**2 for x in series]) / len(series))


def get_descriptive_stats(row: Iterable[Union[str, float]]) -> List[Union[str, float]]:
	"""Get descriptive statistics for a given row."""
	seed_phrase = " ".join(row[:2])
	sentiments = np.array(sorted(row[2:]))
	multistats = MultimodalStats(sentiments)
	triple_pad = lambda x: x + [""] * (3 - len(x))  # Pad list with "" to length 3
	psi = triple_pad(multistats.psi)  # Î¨ values
	deviations = triple_pad(multistats.deviations)  # Individual deviations
	mmdev = np.average(multistats.deviations)  # Multimodal deviation
	zerodev = np.sqrt(sum(np.square(sentiments)) / len(sentiments))  # Deviation of series from 0
	q1 = np.percentile(sentiments, 25, interpolation="midpoint")
	q3 = np.percentile(sentiments, 75, interpolation="midpoint")
	iqr = q3 - q1
	mid_quintile = len(list(filter(lambda x: -0.2 <= x <= 0.2, sentiments))) / len(sentiments)
	mid_third = len(list(filter(lambda x: -0.3333 <= x <= 0.3333, sentiments))) / len(sentiments)
	positivity_rate = sum(sentiments >= 0) / len(sentiments)
	get_placebo_by_rank = lambda i: pdict[seed_phrase][list(sentiments).index(sentiments[i])]
	words_to_ignore = slant.stop_words + row[:2]
	top10_words = slant.top_words("\n".join(map(str, pdict[seed_phrase])), blacklist=words_to_ignore)
	most_polar_placebos = map(get_placebo_by_rank, [0, 1, 2, -1, -2, -3])
	return [
	    *row[:2],
	    np.average(sentiments),
	    np.std(sentiments), *psi, *deviations, mmdev, zerodev, q1,
	    np.median(sentiments), q3, iqr,
	    min(sentiments),
	    max(sentiments), mid_quintile, mid_third, positivity_rate, *most_polar_placebos, *top10_words
	]


# Get descriptive statistics
gpt2_analysis = list(map(get_descriptive_stats, gpt2_sentiments))
gpt3_5_analysis = list(map(get_descriptive_stats, gpt3_5_sentiments))

################################################################################
# Write analysis file
################################################################################
analysis_header = [
    "Identifier0", "Identifier1", "Average", "StandardDeviation", "Peak0", "Peak1", "Peak2",
    "Deviation0", "Deviation1", "Deviation2", "MultimodalDeviation", "ZeroDeviation", "Q1",
    "Median", "Q3", "IQR", "Min", "Max", "MidQuintilePercent", "MidThirdPercent", "PositivityRate",
    "NegativePlacebo0", "NegativePlacebo1", "NegativePlacebo2", "PositivePlacebo0",
    "PositivePlacebo1", "PositivePlacebo2", *[f"FreqWord{i}" for i in range(1, 11)]
]

with open((cwd / "../results/GPT2_placebo_analysis.csv").resolve(), "w") as f:
	csvw = csv.writer(f)
	csvw.writerow(analysis_header)
	csvw.writerows(sorted(gpt2_analysis, key=lambda x: " ".join(x[:2]).lower()))

with open((cwd / "../results/GPT3_5_placebo_analysis.csv").resolve(), "w") as f:
	csvw = csv.writer(f)
	csvw.writerow(analysis_header)
	csvw.writerows(sorted(gpt3_5_analysis, key=lambda x: " ".join(x[:2]).lower()))
